{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AQLM inference example\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/colab_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6egoxPVyckBF"
      },
      "source": [
        "**Install the `aqlm` library**\n",
        "- the only extra dependency to run AQLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A584OAwRWGks"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install aqlm[gpu]==1.0.0dev10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTfcs4lrc1x4"
      },
      "source": [
        "**Load the model as usual**\n",
        "\n",
        "Just don't forget to add:\n",
        " - `trust_remote_code=True` to pull the inference code\n",
        " - `torch_dtype=\"auto\"` to load the model in it's native dtype.\n",
        "\n",
        "The tokenizer is just a normal `Llama 2` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lecaItWkVpIC",
        "outputId": "275559f9-ac77-457d-f6a2-2b4ec2f0f63f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"BlackSamorez/Llama-2-7b-AQLM-2Bit-1x16-hf\", trust_remote_code=True, torch_dtype=\"auto\",\n",
        ").cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BlackSamorez/Llama-2-7b-AQLM-2Bit-1x16-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39QpRiPbcBYa"
      },
      "source": [
        "Do a few forward passes to load CUDA and automatically compile the kernels. It's done separately here for it not to affect the generation speed benchmark below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ii-mWRdQZCOF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "output = quantized_model.generate(tokenizer(\"\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOQfeb_ScIyb"
      },
      "source": [
        "**Measure generation speed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyl4uCxTdmKi",
        "outputId": "9c532f40-61cd-4c3c-d0ea-1ded080b488d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 9.87 s, sys: 165 ms, total: 10 s\n",
            "Wall time: 13.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "output = quantized_model.generate(tokenizer(\"I'm AQLM, \", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=128, max_new_tokens=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5E4tmVdpLF"
      },
      "source": [
        "Note that `transformers` generation is not the fastest implementation and it's heavily influenced by CPU capabilities of _Google Colab_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvShqlguccep"
      },
      "source": [
        "**Check that the output is what one would expect from Llama-2-7b**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsOmDVBvXobJ",
        "outputId": "819f193c-7977-488b-8cc1-bed170d78d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> I'm AQLM, 20 years old, and I'm from the Netherlands. I'm a student and I'm currently studying at the University of Amsterdam. I'm a very active person and I love to meet new people. I'm a very open person and I'm always looking for new things to do. I'm a very active person and I love to meet new people. I'm a very open person and I'm always looking for new things to do. I'm a very active person and I love to meet new people. I'm a very open person and I'm always looking for new\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
