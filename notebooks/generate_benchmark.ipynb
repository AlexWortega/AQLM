{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: TORCH_USE_CUDA_DSA=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0130 00:36:03.164406 594052 utils.py:145] Note: detected 255 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "I0130 00:36:03.166201 594052 utils.py:148] Note: NumExpr detected 255 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "I0130 00:36:03.166588 594052 utils.py:160] NumExpr defaulting to 8 threads.\n",
      "I0130 00:36:03.450076 594052 config.py:58] PyTorch version 2.1.2 available.\n",
      "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n",
      "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n",
      "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env TORCH_USE_CUDA_DSA=True\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/blacksamorez/quip-sharp\")\n",
    "from lib.utils.unsafe_import import model_from_hf_path\n",
    "from model.llama import LlamaForCausalLM as QuipSharpLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0130 00:36:54.963166 594052 warnings.py:109] /home/blacksamorez/quip-sharp/.conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "I0130 00:37:13.597410 594052 modeling.py:920] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a2f10a11f34892a7fdad042390b9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aqlm_model = AutoModelForCausalLM.from_pretrained(\"BlackSamorez/Llama-2-7b-AQLM-6288ppl-hf\", trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=\"auto\").cuda()\n",
    "quip_sharp_model = model_from_hf_path(\n",
    "    \"relaxml/Llama-2-7b-E8P-2Bit\",\n",
    "    use_flash_attn=True,\n",
    "    use_cuda_graph=False,\n",
    ")[0].cuda()\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=\"auto\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQLM:\n",
      " <s> Hi, I'm a newbie here. I'm looking for a good place to learn about the basics of programming.\n",
      "I'm \n",
      "\n",
      "QUIP#:\n",
      " <s> Hi! I'm looking for a new and exciting adventure in my life. hopefully in a few days I will have the chance to join you \n",
      "\n",
      "FP16:\n",
      " <s> Hi everyone, I'm new to the forum.\n",
      "I've been playing with a few different synths lately and I've been having \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = aqlm_model.generate(tokenizer(\"Hi\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=30)\n",
    "print(\"AQLM:\\n\", tokenizer.decode(output[0]), \"\\n\")\n",
    "output = quip_sharp_model.generate(tokenizer(\"Hi\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=30)\n",
    "print(\"QUIP#:\\n\", tokenizer.decode(output[0]), \"\\n\")\n",
    "output = fp16_model.generate(tokenizer(\"Hi\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=30)\n",
    "print(\"FP16:\\n\", tokenizer.decode(output[0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.64 s, sys: 22.9 ms, total: 8.66 s\n",
      "Wall time: 8.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = aqlm_model.generate(tokenizer(\"Hi\", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=200, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 29.2 s, total: 44.4 s\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = quip_sharp_model.generate(tokenizer(\"Hi\", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=200, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.94 s, sys: 5.65 ms, total: 5.95 s\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = fp16_model.generate(tokenizer(\"Hi\", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=200, max_new_tokens=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
